{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image \n",
    "import xml.etree.ElementTree as ET\n",
    "import torch \n",
    "import torch.utils.data \n",
    "from torchvision import transforms as T \n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self done\n"
     ]
    }
   ],
   "source": [
    "#custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotation_dir, transforms = None):\n",
    "        self.img_dir = img_dir \n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.images = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        print(f\"Found {len(self.images)} images\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #load images\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        #load annotation\n",
    "        annotation_file = os.path.join(\n",
    "            self.annotation_dir,\n",
    "            self.images[idx].replace('.jpg' , '.xml').replace('.png', '.xml')\n",
    "        )\n",
    "\n",
    "\n",
    "        #Parse XML annotation\n",
    "\n",
    "        tree = ET.parse(annotation_file)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        #extract bounding boxes and labels\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text \n",
    "\n",
    "            if label == 'with_mask':\n",
    "                label_id = 1 \n",
    "            elif label == 'without_mask':\n",
    "                label_id = 2\n",
    "            else:\n",
    "                label_id = 3\n",
    "\n",
    "            \n",
    "            #get bounding box\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = float(bbox.find('xmin').text)\n",
    "            ymin = float(bbox.find('ymin').text)\n",
    "            xmax = float(bbox.find('xmax').text)\n",
    "            ymax = float(bbox.find('ymax').text)\n",
    "\n",
    "            boxes.append([xmin,ymin,xmax,ymax])\n",
    "            labels.append(label_id)\n",
    "\n",
    "        #convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes \n",
    "        target[\"labels\"] = labels \n",
    "\n",
    "        #transforms\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, target \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "print(\"self done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 853 images\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(\n",
    "    img_dir = r\"C:\\Users\\wming\\OneDrive\\Desktop\\kaggle\\yolov8_detection\\archive\\images\",\n",
    "    annotation_dir= r\"C:\\Users\\wming\\OneDrive\\Desktop\\kaggle\\yolov8_detection\\archive\\annotations\",\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "train_size = int(0.8*len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size,valid_size])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle = True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size = 1,\n",
    "        shuffle = False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wming\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wming\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "num_classes = 4 #background + 3 mask calsses\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"using device: {device}\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU NAME: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/(1024**2):.2f} MB\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train settings\n",
    "optimizer = torch.optim.SGD(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr = 0.001,\n",
    "    momentum = 0.9,\n",
    "    weight_decay= 0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size = 5,\n",
    "    gamma = 0.5\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "best_loss = float('inf')\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with tqdm(data_loader, desc = \"Training\") as pbar:\n",
    "        for images, targets in pbar:\n",
    "            try:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "                if num_batches == 0:\n",
    "                    print(f\"first batch - number of images: {len(images)}\")\n",
    "                    print(f\"first image shape: {images[0].shape}\")\n",
    "                    print(f\"first target boxes shape: {targets[0]['boxes'].shape}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                if not torch.isfinite(losses):\n",
    "                    print(f\"loss is {losses}, skip batch\")\n",
    "                    continue \n",
    "\n",
    "                losses.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm =1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += losses.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'batch_loss': losses.item(),\n",
    "                    'avg_loss' : total_loss / num_batches\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue \n",
    "\n",
    "    return total_loss / num_batches if num_batches > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch - number of images: 1\n",
      "first image shape: torch.Size([3, 210, 400])\n",
      "first target boxes shape: torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 682/682 [24:04<00:00,  2.12s/it, batch_loss=0.129, avg_loss=0.524] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.5238\n",
      "learning rate: 0.001000\n",
      "save best model!\n",
      "\n",
      "Epoch 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch - number of images: 1\n",
      "first image shape: torch.Size([3, 400, 301])\n",
      "first target boxes shape: torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 682/682 [23:42<00:00,  2.09s/it, batch_loss=0.0647, avg_loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.3493\n",
      "learning rate: 0.001000\n",
      "save best model!\n",
      "\n",
      "Epoch 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch - number of images: 1\n",
      "first image shape: torch.Size([3, 400, 267])\n",
      "first target boxes shape: torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 682/682 [23:41<00:00,  2.08s/it, batch_loss=0.232, avg_loss=0.309] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.3095\n",
      "learning rate: 0.001000\n",
      "save best model!\n",
      "\n",
      "Epoch 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch - number of images: 1\n",
      "first image shape: torch.Size([3, 374, 400])\n",
      "first target boxes shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 682/682 [23:43<00:00,  2.09s/it, batch_loss=0.167, avg_loss=0.284] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.2838\n",
      "learning rate: 0.001000\n",
      "save best model!\n",
      "\n",
      "Epoch 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/682 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch - number of images: 1\n",
      "first image shape: torch.Size([3, 400, 267])\n",
      "first target boxes shape: torch.Size([2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 682/682 [23:52<00:00,  2.10s/it, batch_loss=0.0776, avg_loss=0.267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.2671\n",
      "learning rate: 0.000500\n",
      "save best model!\n",
      "\n",
      "Training complete!\n",
      "best loss achieved: 0.2671\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    try:\n",
    "        print(f\"\\nEpoch {epoch+1} / {epochs}\")\n",
    "\n",
    "        epoch_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(f\"average loss: {epoch_loss:.4f}\")\n",
    "        print(f\"learning rate: {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "        #save best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save({\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "\n",
    "            }, 'best_model.pth')\n",
    "            print(\"save best model!\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        break \n",
    "    except Exception as e :\n",
    "        print(f\"Error in epoch {epoch+1}: {e}\")\n",
    "        continue \n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"best loss achieved: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save final model\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': best_loss,\n",
    "}, 'final_model.pth')\n",
    "\n",
    "print('save final model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "targets should not be none when in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m valid_loader:\n\u001b[0;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m---> 18\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i , prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n\u001b[0;32m     21\u001b[0m         image \u001b[38;5;241m=\u001b[39m image[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:2132\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhas_torch_function(\n\u001b[0;32m   2127\u001b[0m     (condition,)\n\u001b[0;32m   2128\u001b[0m ):\n\u001b[0;32m   2129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[0;32m   2130\u001b[0m         _assert, (condition,), condition, message\n\u001b[0;32m   2131\u001b[0m     )\n\u001b[1;32m-> 2132\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[1;31mAssertionError\u001b[0m: targets should not be none when in training mode"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from PIL import Image, ImageDraw \n",
    "def evaluate_model(model, valid_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    label_map = {\n",
    "        1: \"with_mask\",\n",
    "        2: \"without_mask\",\n",
    "        3: \"mask_incorrect\"\n",
    "    }\n",
    "\n",
    "#Test on validation dataset\n",
    "with torch.no_grad(): #disable gradient calculations\n",
    "    for images, targets in valid_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        predictions = model(images)\n",
    "\n",
    "        for i , prediction in enumerate(predictions):\n",
    "            image = image[i].cpu().permute(1,2,0).numpy()\n",
    "            image = (image*255).astype(np.uint8)\n",
    "\n",
    "            pil_image = Image.fromarray(image)\n",
    "            draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "            boxes = prediction['boxes'].cpu().numpy()\n",
    "            labels = prediction['labels'].cpu().numpy()\n",
    "            scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if score > 0.5: #threshold\n",
    "                    box = box.astype(np.int32)\n",
    "                    draw.rectangle(\n",
    "                        [(box[0], box[1], box[2], box[3])],\n",
    "                        outline = 'red',\n",
    "                        width = 2\n",
    "                    )\n",
    "\n",
    "                    label_text = f\"{label_map[label]}: {score:.2f}\"\n",
    "                    draw.text((box[0], box[1] - 10,), label_text, fill = 'blue')\n",
    "            pil_image.show() #for display\n",
    "            pil_image.save(f\"prediction_{i}.png\")\n",
    "\n",
    "\n",
    "\n",
    "            #print predictions\n",
    "\n",
    "            print(f\"\\nImage {i} Predictions:\")\n",
    "            print(\"Boxes\", boxes[scores>0.5])\n",
    "            print(\"Labels:\", [label_map[l] for l in labels[score > 0.5]])\n",
    "            print(\"Scores:\", scores[score > 0.5])\n",
    "        break \n",
    "\n",
    "\n",
    "#use the function\n",
    "print(\"staring evaluation...\")\n",
    "evaluate_model(model, valid_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(mode, valid_loader, device, num_images = 120):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    colors = {\n",
    "        'with_mask': (0,255,0),\n",
    "        'without_mask': (255,0,0),\n",
    "        'mask_incorrect': (0,0,255)\n",
    "    }\n",
    "\n",
    "    label_map = {\n",
    "        1: 'with_mask',\n",
    "        2: 'without_mask',\n",
    "        3: 'mask_incorrect'\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images,targets) in enumerate(valid_loader):\n",
    "            # move images to device\n",
    "\n",
    "            images = list(img.to(device) for img in images)\n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            for i , (image,prediction) in enumerate(zip(images, predictions)):\n",
    "\n",
    "                if batch_idx * len(images) + i >= num_images:\n",
    "                    return \n",
    "                \n",
    "                img_np = image.cpu().permute(1,2,0).numpy()\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                boxes = prediction['boxes'].cpu().numpy()\n",
    "                labels = prediction['labels'].cpu().numpy()\n",
    "                scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "                for box, label, score in zip(boxes, labels, scores):\n",
    "                    if score >0.5:\n",
    "                        box = box.astype(int)\n",
    "                        label_name = label_map[label]\n",
    "                        color = colors[label_name]\n",
    "\n",
    "                        cv2.rectangle(imp_np, (box[0], box[1]), (box[2], box[3]). color, 3)\n",
    "                        label_text = f\"{label_name}: {score:.2f}\"\n",
    "                        cv2.putText(img_np, label_text, (box[0], box[1] - 10),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                        \n",
    "                cv2.imwrite(f\"prediction_image_{batch_idx}_{i}.png\" , img_np)\n",
    "                print(f\"saved prediction_image_{batch_idx}_{i}.png\")\n",
    "\n",
    "print(\"starting visualization...\")\n",
    "visualize_predictions(model, valid_loader, device, num_images= 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display \n",
    "import glob \n",
    "\n",
    "for image_path in sorted(glob.glob('prediction_image_*.png')):\n",
    "    display(Image(filename = image_path))\n",
    "    print(f\"displaying {iamge_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
